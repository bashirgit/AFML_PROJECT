{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386d8591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mluser/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# ESM2 MASKED LANGUAGE MODEL TRAINING + EVALUATION\n",
    "# ===========================================================\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from itertools import product  # Import 'product'\n",
    "import logging\n",
    "\n",
    "# Suppress excessive logging from transformers\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c24e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/mluser/.venv/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mluser/.venv/lib/python3.12/site-packages (from accelerate) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mluser/.venv/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /home/mluser/.venv/lib/python3.12/site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/mluser/.venv/lib/python3.12/site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/mluser/.venv/lib/python3.12/site-packages (from accelerate) (2.9.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/mluser/.venv/lib/python3.12/site-packages (from accelerate) (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/mluser/.venv/lib/python3.12/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.9.0)\n",
      "Requirement already satisfied: requests in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/mluser/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mluser/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "PyTorch Version: 2.9.0+cu128\n",
      "CUDA Available: True\n",
      "Device Name: NVIDIA GeForce RTX 4090\n",
      "PyTorch built with CUDA Version: 12.8\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate --upgrade\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"PyTorch built with CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ccae84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 7989 sequences from /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_train_10k.fasta\n",
      " Loaded 1002 sequences from /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_val_10k.fasta\n",
      " Loaded 1009 sequences from /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_test_10k.fasta\n",
      "\n",
      "Training data sample:\n",
      "                                            sequence\n",
      "0  MHWLYPSFSSSSSSSPAPSTADDNYINNRTSVAGSHHRRRMTENDV...\n",
      "1  MHYATSDYENGNRTTMNSPSSVMHKDQSPVLTPRKPREALRPANLL...\n",
      "2  MATSFLEQYDALDVIGNGSFGIIRKVRRKADGSIFARKELNFERMS...\n",
      "3  MTQFLELYEPLDIIGNGSFGIIRKVKRKMDGMIFARKELNFERMSE...\n",
      "4  MNNDEEKIQDISKKIEREKALINAANLMRQQTNNEAVRSKLDTQMR...\n"
     ]
    }
   ],
   "source": [
    "def load_fasta_file(file_path):\n",
    "    \"\"\"\n",
    "    Loads a single FASTA file into a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        seq = str(record.seq)\n",
    "        if len(seq) > 0:\n",
    "            sequences.append(seq)\n",
    "            \n",
    "    print(f\" Loaded {len(sequences)} sequences from {file_path}\")\n",
    "    return pd.DataFrame({\"sequence\": sequences})\n",
    "\n",
    "# --- Load all data ---\n",
    "# !!! UPDATE THIS PATH TO MATCH YOUR SYSTEM !!!\n",
    "base_path = r\"/home/mluser/AFML_RISHABH/Project/10k sequences\"\n",
    "train_df = load_fasta_file(os.path.join(base_path, \"kinases_cluster_train_10k.fasta\"))\n",
    "val_df   = load_fasta_file(os.path.join(base_path, \"kinases_cluster_val_10k.fasta\"))\n",
    "test_df  = load_fasta_file(os.path.join(base_path, \"kinases_cluster_test_10k.fasta\"))\n",
    "\n",
    "print(\"\\nTraining data sample:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6bddb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 7989/7989 [00:04<00:00, 1878.45 examples/s]\n",
      "Map (num_proc=2): 100%|██████████| 1002/1002 [00:00<00:00, 1319.12 examples/s]\n",
      "Map (num_proc=2): 100%|██████████| 1009/1009 [00:00<00:00, 1363.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tokenization complete.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'special_tokens_mask', 'attention_mask'],\n",
      "        num_rows: 7989\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'special_tokens_mask', 'attention_mask'],\n",
      "        num_rows: 1002\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'special_tokens_mask', 'attention_mask'],\n",
      "        num_rows: 1009\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/esm2_t12_35M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "fixed_max_length = 512 # This is from your hyperparameter list\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    spaced_sequences = [\" \".join(list(seq)) for seq in examples[\"sequence\"]]\n",
    "    \n",
    "    return tokenizer(\n",
    "        spaced_sequences,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=fixed_max_length,  # Use the fixed max length\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "# --- Create datasets ---\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset   = Dataset.from_pandas(val_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "\n",
    "# --- Tokenize datasets ---\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    num_proc=2, \n",
    "    remove_columns=[\"sequence\"],\n",
    "    fn_kwargs={'tokenizer': tokenizer}\n",
    ")\n",
    "tokenized_val   = val_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    num_proc=2, \n",
    "    remove_columns=[\"sequence\"],\n",
    "    fn_kwargs={'tokenizer': tokenizer}\n",
    ")\n",
    "tokenized_test  = test_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    num_proc=2, \n",
    "    remove_columns=[\"sequence\"],\n",
    "    fn_kwargs={'tokenizer': tokenizer}\n",
    ")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": tokenized_train,\n",
    "    \"validation\": tokenized_val,\n",
    "    \"test\": tokenized_test\n",
    "})\n",
    "\n",
    "print(\"\\n Tokenization complete.\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b68b5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data collator ready.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "print(\" Data collator ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79c9a6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total combinations: 24\n",
      " Starting fresh — no previous runs found.\n",
      "\n",
      "====================================================================================================\n",
      " Starting Run 1/24 → run_001_lr3e-05_wd0.01_bs1_ga4_len512\n",
      "====================================================================================================\n",
      " Cleared accelerator state\n",
      " Cleared GPU cache\n",
      " Training model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19980' max='19980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19980/19980 15:16, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.135800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.090200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.111700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.096300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.066200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.083500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.062600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.994600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.996700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.969200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.979500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>2.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.990200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.949000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.979700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.960100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.986500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.974800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.962100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.957200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.931100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.958100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.953500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.937800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.936100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.928900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.965400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.935700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.931100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.949900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.953200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.951800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.942500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.962200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.928600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.929800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>1.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.929400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>1.917500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>1.902600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>1.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>1.916900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.910200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>1.934300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>1.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>1.926600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>1.941900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.922800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>1.911200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>1.909500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>1.905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>1.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.908200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>1.899000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>1.940300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>1.926300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>1.898500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.923300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>1.920500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>1.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>1.906800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>1.893200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.886900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>1.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>1.916800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>1.920100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>1.919300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>1.912600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>1.898600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>1.893900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>1.902400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.907400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>1.906600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>1.910800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>1.885400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>1.905900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.941800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>1.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>1.886900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>1.902400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>1.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.878100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>1.903300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>1.893200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>1.926800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>1.914100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.887700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>1.890200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>1.867400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>1.879100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>1.877200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.898200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>1.873200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>1.898400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>1.885000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>1.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.902200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>1.897900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>1.888200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>1.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>1.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.888700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>1.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>1.904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>1.861500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>1.873600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.881700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>1.879100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>1.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>1.880500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>1.909600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All checkpoints & logs saved to /home/mluser/AFML_RISHABH/Project/hyperparam_runs_esm2/run_001_lr3e-05_wd0.01_bs1_ga4_len512\n",
      " Updated master summary → /home/mluser/AFML_RISHABH/Project/hyperparam_runs_esm2/esm2_training_only_summary.csv\n",
      " GPU memory cleared.\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      " Starting Run 2/24 → run_002_lr3e-05_wd0.01_bs1_ga8_len512\n",
      "====================================================================================================\n",
      " Cleared accelerator state\n",
      " Cleared GPU cache\n",
      " Training model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='483' max='9990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 483/9990 00:43 < 14:26, 10.98 it/s, Epoch 0.48/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.140800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updated master summary → /home/mluser/AFML_RISHABH/Project/hyperparam_runs_esm2/esm2_training_only_summary.csv\n",
      " GPU memory cleared.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 116\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    115\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Training model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     status = \u001b[33m\"\u001b[39m\u001b[33mSuccess\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m     trainer.save_model(run_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/accelerate/data_loader.py:579\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    577\u001b[39m     current_batch = send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m next_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_index >= \u001b[38;5;28mself\u001b[39m.skip_batches:\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2863\u001b[39m, in \u001b[36mDataset.__getitems__\u001b[39m\u001b[34m(self, keys)\u001b[39m\n\u001b[32m   2861\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: \u001b[38;5;28mlist\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m   2862\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2863\u001b[39m     batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2864\u001b[39m     n_examples = \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[32m   2865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch.items()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2859\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2857\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33marrow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpolars\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2858\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[32m-> \u001b[39m\u001b[32m2859\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2841\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2839\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2840\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2841\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2844\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:658\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    656\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:415\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_column(pa_table)\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:471\u001b[39m, in \u001b[36mPythonFormatter.format_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lazy:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_batch(batch)\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:150\u001b[39m, in \u001b[36mPythonArrowExtractor.extract_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os, gc, torch, pandas as pd\n",
    "from itertools import product\n",
    "from contextlib import nullcontext\n",
    "from transformers import AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "from accelerate.state import AcceleratorState\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "BASE_DIR = \"/home/mluser/AFML_RISHABH/Project/hyperparam_runs_esm2\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "MASTER_CSV = os.path.join(BASE_DIR, \"esm2_training_only_summary.csv\")\n",
    "\n",
    "# === Hyperparameter Ranges ===\n",
    "learning_rates = [3e-5, 5e-5, 7e-5]\n",
    "weight_decays = [0.01, 0.05]\n",
    "batch_sizes = [1, 2]\n",
    "grad_accum_steps = [4, 8]\n",
    "max_seq_lens = [512]\n",
    "num_train_epochs = 10\n",
    "save_steps = 1000  #  Checkpoint every 1000 steps\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "\n",
    "# === Generate combinations ===\n",
    "hyperparameter_combinations = list(product(\n",
    "    learning_rates, weight_decays, batch_sizes, grad_accum_steps, max_seq_lens\n",
    "))\n",
    "print(f\" Total combinations: {len(hyperparameter_combinations)}\")\n",
    "\n",
    "# === Load completed runs (resume safe) ===\n",
    "results, completed = [], set()\n",
    "if os.path.exists(MASTER_CSV):\n",
    "    prev_df = pd.read_csv(MASTER_CSV)\n",
    "    for _, row in prev_df.iterrows():\n",
    "        combo = (row[\"learning_rate\"], row[\"weight_decay\"], row[\"batch_size\"], row[\"grad_accum_steps\"], row[\"max_seq_len\"])\n",
    "        completed.add(combo)\n",
    "    results = prev_df.to_dict(orient=\"records\")\n",
    "    print(f\" Loaded {len(completed)} completed runs from {MASTER_CSV}\")\n",
    "else:\n",
    "    print(\" Starting fresh — no previous runs found.\")\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for i, (lr, wd, bs, gas, msl) in enumerate(hyperparameter_combinations, 1):\n",
    "    combo = (lr, wd, bs, gas, msl)\n",
    "    if combo in completed:\n",
    "        print(f\" Skipping Run {i}: already done (lr={lr}, wd={wd}, bs={bs}, gas={gas}, len={msl})\")\n",
    "        continue\n",
    "\n",
    "    run_name = f\"run_{i:03d}_lr{lr}_wd{wd}_bs{bs}_ga{gas}_len{msl}\"\n",
    "    run_dir = os.path.join(BASE_DIR, run_name)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\" Starting Run {i}/{len(hyperparameter_combinations)} → {run_name}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    # --- Reset accelerator ---\n",
    "    try:\n",
    "        if hasattr(AcceleratorState, \"_shared_state\") and AcceleratorState._shared_state:\n",
    "            AcceleratorState._reset_state()\n",
    "            print(\" Cleared accelerator state\")\n",
    "    except Exception as e:\n",
    "        print(f\" Could not clear accelerator state: {e}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\" Cleared GPU cache\")\n",
    "\n",
    "    # --- Initialize Model ---\n",
    "    try:\n",
    "        model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to load model: {e}\")\n",
    "        results.append({\n",
    "            \"run_id\": i, \"learning_rate\": lr, \"weight_decay\": wd,\n",
    "            \"batch_size\": bs, \"grad_accum_steps\": gas,\n",
    "            \"max_seq_len\": msl, \"output_dir\": run_dir, \"status\": \"ModelLoadFailed\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # --- Training Arguments ---\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=run_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=bs,\n",
    "        gradient_accumulation_steps=gas,\n",
    "        learning_rate=lr,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=wd,\n",
    "        logging_dir=os.path.join(BASE_DIR, \"logs\", run_name),\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"steps\",     #  Save every N steps\n",
    "        save_steps=save_steps,     #  1000-step checkpointing\n",
    "        save_total_limit=None,     #  Keep all checkpoints\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # --- Trainer ---\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    trainer.autocast_smart_context_manager = nullcontext\n",
    "\n",
    "    # --- Train + Save ---\n",
    "    try:\n",
    "        print(\" Training model...\")\n",
    "        train_result = trainer.train()\n",
    "        status = \"Success\"\n",
    "\n",
    "        trainer.save_model(run_dir)\n",
    "        pd.DataFrame(trainer.state.log_history).to_csv(os.path.join(run_dir, \"log_history.csv\"), index=False)\n",
    "        print(f\" All checkpoints & logs saved to {run_dir}\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        status = \"OOM\" if \"out of memory\" in str(e).lower() else \"Failed\"\n",
    "        print(f\" Run {i} failed: {status}\")\n",
    "    except Exception as e:\n",
    "        status = f\"Error: {str(e)[:80]}\"\n",
    "        print(f\" Run {i} crashed: {e}\")\n",
    "    finally:\n",
    "        # Log run result\n",
    "        results.append({\n",
    "            \"run_id\": i,\n",
    "            \"learning_rate\": lr,\n",
    "            \"weight_decay\": wd,\n",
    "            \"batch_size\": bs,\n",
    "            \"grad_accum_steps\": gas,\n",
    "            \"max_seq_len\": msl,\n",
    "            \"output_dir\": run_dir,\n",
    "            \"status\": status,\n",
    "        })\n",
    "        completed.add(combo)\n",
    "        pd.DataFrame(results).to_csv(MASTER_CSV, index=False)\n",
    "        print(f\" Updated master summary → {MASTER_CSV}\")\n",
    "\n",
    "        # Free memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        print(\" GPU memory cleared.\\n\")\n",
    "\n",
    "print(\"\\n All hyperparameter runs completed!\")\n",
    "print(f\" Master summary available at: {MASTER_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bba18584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Combined eval samples: 2011\n",
      "\n",
      " Found 2 runs to evaluate.\n",
      "\n",
      "================================================================================\n",
      " Evaluating Run 1/2 → run_001_lr3e-05_wd0.01_bs1_ga4_len512\n",
      "================================================================================\n",
      " Loading latest checkpoint: /home/mluser/AFML_RISHABH/Project/hyperparam_runs_esm2/run_001_lr3e-05_wd0.01_bs1_ga4_len512/checkpoint-19980\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " run_001_lr3e-05_wd0.01_bs1_ga4_len512 → Eval Loss: 2.0784, Perplexity: 7.99\n",
      " Cleared GPU and memory.\n",
      "\n",
      "================================================================================\n",
      " Evaluating Run 2/2 → run_002_lr3e-05_wd0.01_bs1_ga8_len512\n",
      "================================================================================\n",
      " Evaluation failed for run_002_lr3e-05_wd0.01_bs1_ga8_len512: No checkpoints found in this run directory.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     79\u001b[39m     results.append({\n\u001b[32m     80\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m     81\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcheckpoint\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)[:\u001b[32m120\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     85\u001b[39m     })\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtrainer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[32m     90\u001b[39m         \u001b[38;5;28;01mdel\u001b[39;00m trainer\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import os, gc, torch, pandas as pd\n",
    "from transformers import AutoModelForMaskedLM, Trainer\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# ===== CONFIG =====\n",
    "BASE_DIR = \"/home/mluser/AFML_RISHABH/Project/hyperparam_runs_esm2\"\n",
    "SUMMARY_CSV = os.path.join(BASE_DIR, \"esm2_evaluation_results.csv\")\n",
    "\n",
    "# === Dataset & Data Collator should already exist ===\n",
    "# Must define `dataset[\"validation\"]` and `dataset[\"test\"]`\n",
    "# and a compatible `data_collator`\n",
    "\n",
    "# --- Combine validation and test datasets ---\n",
    "combined_eval_dataset = concatenate_datasets([\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"]\n",
    "])\n",
    "print(f\" Combined eval samples: {len(combined_eval_dataset)}\\n\")\n",
    "\n",
    "# === Find all run directories ===\n",
    "run_dirs = sorted([\n",
    "    d for d in os.listdir(BASE_DIR)\n",
    "    if d.startswith(\"run_\") and os.path.isdir(os.path.join(BASE_DIR, d))\n",
    "])\n",
    "print(f\" Found {len(run_dirs)} runs to evaluate.\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, run_name in enumerate(run_dirs, 1):\n",
    "    run_dir = os.path.join(BASE_DIR, run_name)\n",
    "    print(\"=\" * 80)\n",
    "    print(f\" Evaluating Run {i}/{len(run_dirs)} → {run_name}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        # --- Find the latest checkpoint ---\n",
    "        ckpts = [\n",
    "            d for d in os.listdir(run_dir)\n",
    "            if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(run_dir, d))\n",
    "        ]\n",
    "        if not ckpts:\n",
    "            raise FileNotFoundError(\"No checkpoints found in this run directory.\")\n",
    "\n",
    "        ckpts_sorted = sorted(ckpts, key=lambda x: int(x.split('-')[-1]))\n",
    "        latest_ckpt = os.path.join(run_dir, ckpts_sorted[-1])\n",
    "        print(f\" Loading latest checkpoint: {latest_ckpt}\")\n",
    "\n",
    "        # --- Load model ---\n",
    "        model = AutoModelForMaskedLM.from_pretrained(latest_ckpt)\n",
    "        model.eval()\n",
    "\n",
    "        # --- Evaluate ---\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            eval_dataset=combined_eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        eval_results = trainer.evaluate()\n",
    "        loss = eval_results.get(\"eval_loss\", None)\n",
    "        perplexity = torch.exp(torch.tensor(loss)).item() if loss is not None else None\n",
    "\n",
    "        print(f\" {run_name} → Eval Loss: {loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"run_name\": run_name,\n",
    "            \"checkpoint\": ckpts_sorted[-1],\n",
    "            \"eval_loss\": loss,\n",
    "            \"perplexity\": perplexity,\n",
    "            \"status\": \"Success\",\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Evaluation failed for {run_name}: {e}\")\n",
    "        results.append({\n",
    "            \"run_name\": run_name,\n",
    "            \"checkpoint\": None,\n",
    "            \"eval_loss\": None,\n",
    "            \"perplexity\": None,\n",
    "            \"status\": f\"Failed: {str(e)[:120]}\",\n",
    "        })\n",
    "\n",
    "    finally:\n",
    "        del model\n",
    "        if \"trainer\" in locals():\n",
    "            del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\" Cleared GPU and memory.\\n\")\n",
    "\n",
    "# === Save Results ===\n",
    "df = pd.DataFrame(results)\n",
    "if len(df) > 0:\n",
    "    df.to_csv(SUMMARY_CSV, index=False)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\" All Evaluations Complete \")\n",
    "    print(f\" Results saved to: {SUMMARY_CSV}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"\\n No evaluation results were collected. Check dataset paths or run directories.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23ae16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
