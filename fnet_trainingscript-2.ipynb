{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a335e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in /home/mluser/.venv/lib/python3.12/site-packages (1.85)\n",
      "Requirement already satisfied: transformers in /home/mluser/.venv/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /home/mluser/.venv/lib/python3.12/site-packages (2.9.0)\n",
      "Requirement already satisfied: datasets in /home/mluser/.venv/lib/python3.12/site-packages (4.2.0)\n",
      "Requirement already satisfied: numpy in /home/mluser/.venv/lib/python3.12/site-packages (2.3.4)\n",
      "Requirement already satisfied: scikit-learn in /home/mluser/.venv/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: evaluate in /home/mluser/.venv/lib/python3.12/site-packages (0.4.6)\n",
      "Requirement already satisfied: filelock in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/mluser/.venv/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/mluser/.venv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/mluser/.venv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/mluser/.venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/mluser/.venv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/mluser/.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/mluser/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: anyio in /home/mluser/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/mluser/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/mluser/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/mluser/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/mluser/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/mluser/.venv/lib/python3.12/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/mluser/.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/mluser/.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/mluser/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/mluser/.venv/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mluser/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mluser/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mluser/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/mluser/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/mluser/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install biopython transformers torch datasets numpy scikit-learn evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6020c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /home/mluser/.venv/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.2 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (2.9.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (1.10.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.1.10)\n",
      "Requirement already satisfied: psutil in /home/mluser/.venv/lib/python3.12/site-packages (from accelerate>=0.26.0->transformers[torch]) (7.1.0)\n",
      "Requirement already satisfied: setuptools in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/mluser/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mluser/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59246fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mluser/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "import evaluate\n",
    "import inspect\n",
    "\n",
    "print(\"✅ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e895d205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU cache cleared\n",
      "   GPU: NVIDIA GeForce RTX 4090\n",
      "   Total GPU memory: 25.25 GB\n",
      "   Available memory: 25.25 GB\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache before starting\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"✅ GPU cache cleared\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   Available memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca836fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/fnet-base\")\n",
    "print(\"✅ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3750af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FASTADataset class defined\n"
     ]
    }
   ],
   "source": [
    "class FASTADataset(Dataset):\n",
    "    def __init__(self, fasta_file, tokenizer, max_length=512):\n",
    "        print(f\"Loading sequences from {fasta_file}...\")\n",
    "        self.sequences = []\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            seq = str(record.seq)\n",
    "            if len(seq) > 0:\n",
    "                self.sequences.append(seq)\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        print(f\"Loaded {len(self.sequences)} sequences\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        spaced_seq = \" \".join(list(seq))\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            spaced_seq,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        result = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        if 'attention_mask' not in result:\n",
    "            result['attention_mask'] = torch.ones_like(result['input_ids'])\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"✅ FASTADataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2735ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found: /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_train_10k.fasta\n",
      "✓ Found: /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_val_10k.fasta\n",
      "✓ Found: /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_test_10k.fasta\n",
      "Loading sequences from /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_train_10k.fasta...\n",
      "Loaded 7989 sequences\n",
      "Loading sequences from /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_val_10k.fasta...\n",
      "Loaded 1002 sequences\n",
      "Loading sequences from /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_test_10k.fasta...\n",
      "Loaded 1009 sequences\n",
      "\n",
      "==================================================\n",
      "Dataset sizes: 7989, 1002, 1009\n",
      "Max sequence length: 256\n",
      "==================================================\n",
      "\n",
      "Testing dataset[0]...\n",
      "✓ Sample retrieved successfully\n",
      "  Keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "  input_ids shape: torch.Size([256])\n",
      "  First 20 tokens: [4, 94, 123, 100, 153, 266, 101, 66, 129, 66, 66, 66, 66, 66, 66, 66, 101, 70, 101, 66]\n"
     ]
    }
   ],
   "source": [
    "data_folder = '/home/mluser/AFML_RISHABH/Project/10k sequences'\n",
    "\n",
    "train_path = os.path.join(data_folder, \"kinases_cluster_train_10k.fasta\")\n",
    "val_path   = os.path.join(data_folder, \"kinases_cluster_val_10k.fasta\")\n",
    "test_path  = os.path.join(data_folder, \"kinases_cluster_test_10k.fasta\")\n",
    "\n",
    "# Verify files exist\n",
    "for path in [train_path, val_path, test_path]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    print(f\"✓ Found: {path}\")\n",
    "\n",
    "# Use reduced max_length to save memory\n",
    "MAX_LENGTH = 256  # Reduced from 512\n",
    "\n",
    "train_dataset = FASTADataset(train_path, tokenizer, max_length=MAX_LENGTH)\n",
    "val_dataset   = FASTADataset(val_path, tokenizer, max_length=MAX_LENGTH)\n",
    "test_dataset  = FASTADataset(test_path, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Dataset sizes: {len(train_dataset)}, {len(val_dataset)}, {len(test_dataset)}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test dataset\n",
    "print(\"\\nTesting dataset[0]...\")\n",
    "sample = train_dataset[0]\n",
    "print(\"✓ Sample retrieved successfully\")\n",
    "print(f\"  Keys: {sample.keys()}\")\n",
    "print(f\"  input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"  First 20 tokens: {sample['input_ids'][:20].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e4fa0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gradient checkpointing enabled\n",
      "✅ Model ready on cuda\n",
      "   GPU memory allocated: 0.33 GB\n",
      "   GPU memory reserved: 0.37 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mluser/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.fnet.modeling_fnet import FNetBasicFourierTransform\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"google/fnet-base\")\n",
    "\n",
    "# Patch FNet Fourier Transform for float32\n",
    "class FNetSafeFourierTransform(FNetBasicFourierTransform):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        outputs = torch.fft.fftn(hidden_states, dim=(-2, -1)).real\n",
    "        return (outputs,)\n",
    "\n",
    "model.fourier_transform = FNetSafeFourierTransform(model.config)\n",
    "\n",
    "# Force FP32 and handle unexpected kwargs\n",
    "def force_fp32_forward(original_forward):\n",
    "    sig = inspect.signature(original_forward)\n",
    "    \n",
    "    def wrapped_forward(*args, **kwargs):\n",
    "        valid_params = set(sig.parameters.keys())\n",
    "        filtered_kwargs = {k: v for k, v in kwargs.items() if k in valid_params}\n",
    "        \n",
    "        with torch.autocast(device_type='cuda', enabled=False):\n",
    "            return original_forward(*args, **filtered_kwargs)\n",
    "    \n",
    "    return wrapped_forward\n",
    "\n",
    "model.forward = force_fp32_forward(model.forward)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).to(torch.float32)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"✓ Gradient checkpointing enabled\")\n",
    "\n",
    "print(f\"✅ Model ready on {device}\")\n",
    "\n",
    "# Check memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"   GPU memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2700607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data collator...\n",
      "✓ Collated batch keys: KeysView({'input_ids': tensor([[    4,    94,   123,   100,   153,   266,   101,    66,   129,    66,\n",
      "            66,     6,    66,    66,    66,    66,   101,     6,   101,    66,\n",
      "            54,    70,   125,   125,   172,     6,    57,   172,   172,   145,\n",
      "            54,    66,   334,    70,   162,    66,     6,   123,     6,   145,\n",
      "             6,    94,    54,   164,   172,   125,   334,   334,     6,    70,\n",
      "           101,    66,   145,    66,    57,    66,    54,   101,   123,    54,\n",
      "           153,   101,     6,   101,   125,   153,    66,    70,   153,   129,\n",
      "           307,    66,   164,  2437,    70,     6,   162,   125,     6,     6,\n",
      "           153,   172,   266,    57,   153,   101,    66,   101,   334,  1007,\n",
      "           101,   172,   145,     6,    66,     6,   125,     6,   172,   145,\n",
      "           125,     6,   307,   145,   101,   101,   162,   153,   162,    66,\n",
      "           153,     6,   145,    66,   145,   172,     6,   307,   123,   307,\n",
      "           164,    66,     6,   129,   172,   153,     6,   100,    70,  1007,\n",
      "             6,   129,    54,    66,     6,    70,   162,    66,     6,   125,\n",
      "           334,   266,   172,   123,  2841,     6,    54,   334,    66,   125,\n",
      "           101,     6,    54,   101,   334,     6,    66,     6,    70,   153,\n",
      "           153,   101,   307,    94,   125,    94,    66,   101,     6,     6,\n",
      "          1007,   153,     6,    66,   334,   153,   162,     6,    54,   101,\n",
      "           153,  1007,   125,    94,    54,   101,     6,   334,   162,   123,\n",
      "             6,     6,     6,   123,    54,    66,  1007,   145,    66,   334,\n",
      "           123,   162,   129,   125,   162,   153,    66,   334,   101,    66,\n",
      "           101,   164,   145,   145,    54,   145,   334,     6,    66,   101,\n",
      "            94,     6,    66,    76,    66,   101,   153,   123,   307,   307,\n",
      "            94,   101,   129,     6,   162,     6,    94,    66,   145,   334,\n",
      "           125,    66,   172,     6,  1007,    70,    70,   334,    54,    54,\n",
      "           334,   123,   101,   153,   101,     5],\n",
      "        [    4,    94,   123,   266,    70,    54,    66,   125,   266,   164,\n",
      "           172,   162,   172,   145,     6,    54,    94,   172,    66,   101,\n",
      "            66,    66,     6,    94,   123,   307,   125,  1007,    66,   101,\n",
      "           334,   153,    54,   101,   145,   307,   101,   145,   164,     6,\n",
      "           153,   145,   101,    70,   172,     6,   153,   153,   172,   172,\n",
      "          1007,    57,   153,    66,    54,    66,   101,   307,   100,   334,\n",
      "           101,   153,    66,    54,     6,   334,   101,     6,   153,   101,\n",
      "           334,   101,  1007,     6, 30784,    70,   129,    76,    66,     6,\n",
      "           101,   125,     6,    66,    94,    66,    66,     6,    66,   145,\n",
      "            66,   101,    94,   145,   153,   129,    66,   101,   164,  1007,\n",
      "           101,   266,    70,   125,    57,    70,   153,   153,   162,    66,\n",
      "           162,     6,    76,    66,    66,   101,   162,     6,   162,     6,\n",
      "           172,    66,     6,   123,     6,    66,    57,   162,     6,   125,\n",
      "            94,     6,   145,  1007,   153,   129,     6,   101,   123,    66,\n",
      "           145,    76,    66,   101,   164,    76,    66,   101,    57,   101,\n",
      "            66,   101,     6,    94,    54,    66,     6,   162,   101,    66,\n",
      "            66,   145,     6,  1007,    66,   162,    70,     6,    54,   101,\n",
      "           153,   123,   101,   145,    70,   162,   334,    70,    70,    70,\n",
      "           164,    66,    70,    54,   172,   145,   101,   125,   125,   162,\n",
      "           307, 21116,  1007,    66,   123,     6,   153,  8277,   153,   101,\n",
      "           153,    57,    54,    57,    66,   172,     6,    76,   101,   129,\n",
      "            66,   101,     6,   266,    66,    54,    66,    54,    54,   101,\n",
      "            66,   334,    66,  1007,    66,   101,   162,   145,    70,   164,\n",
      "             6,   101,    57,    66,   101,   162,    66,     6,     6,   307,\n",
      "           307,   162,   145,     6,   153,   162,   145,     6,    54,   129,\n",
      "           162,   123,   334,   266,   153,     5]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,   66,\n",
      "         -100, -100, -100, -100, -100,   70, -100,   66, -100, -100, -100, -100,\n",
      "         -100,  266, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          123, -100,  145, -100,  145, -100, -100, -100, -100,  125, -100, -100,\n",
      "          145, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100,  153, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100,   66,   70,  162, -100, -100,  172,  307, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100,  145, -100,  125,\n",
      "         -100,  172, -100, -100, -100,  153, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100,  172, -100, -100, -100, -100,   54, -100, -100, -100,\n",
      "         -100, -100, 1007, -100, -100, -100,  307, -100, -100, -100,  162, -100,\n",
      "         -100, -100,  145, -100, -100, -100,  334, -100, -100, -100, -100, -100,\n",
      "          162,  164, -100, -100, -100, -100, -100,   54, -100, -100, -100,  153,\n",
      "         -100, 1007, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          162,   54, -100, -100,  145, -100, -100, -100, -100,   54, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100,  162, -100, -100, -100,  123,  123,\n",
      "          129, -100, -100, -100, -100, -100, -100,  334, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100,  307, -100, -100, -100, 1007, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100,  164, -100,  101, -100, -100, -100, -100,\n",
      "         -100, -100, -100,   70, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100],\n",
      "        [-100, -100,  123, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100,   54, -100, -100, -100, -100, -100, -100, -100,  334, -100,\n",
      "         -100,  307, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100,   70, -100, -100, -100, -100, -100,  153, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100,  123, -100, -100,  172, -100, -100, -100, -100,\n",
      "         -100,  172,  162, -100, -100, -100, -100,   70, -100, -100,   66, -100,\n",
      "         -100, -100, -100,  101, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100,  123, -100, -100, -100, -100, -100,   66, -100,  123,\n",
      "         -100, -100,  162, -100,  172, -100, -100, -100,  162, -100, -100,   66,\n",
      "         -100, -100, -100, -100,  145, -100, -100,   66, -100, -100, -100, -100,\n",
      "         -100, -100,   66, -100, -100, -100, -100, -100,  145, -100, -100, -100,\n",
      "          101, -100, -100, -100, -100, -100,   57, -100, -100, -100, -100,  334,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100,  334, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1007,\n",
      "         -100, -100, -100,  145, -100,  101, -100, -100, -100, -100,   54, -100,\n",
      "         -100, -100,   66, -100, -100, -100, -100, -100,   54, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100,  164,  172, -100, -100, -100, -100, -100, -100,  145,  100, -100,\n",
      "         -100, -100, -100,  153, -100, -100, -100,  162, -100,  129, -100, -100,\n",
      "         -100, -100, -100, -100]])})\n",
      "  input_ids shape: torch.Size([2, 256])\n",
      "  labels shape: torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Test the data collator\n",
    "print(\"Testing data collator...\")\n",
    "batch = [train_dataset[i] for i in range(2)]\n",
    "collated = data_collator(batch)\n",
    "print(f\"✓ Collated batch keys: {collated.keys()}\")\n",
    "print(f\"  input_ids shape: {collated['input_ids'].shape}\")\n",
    "print(f\"  labels shape: {collated['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a97ac80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./KinaseFNet_10k\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=1,  # Batch size 1\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # Keep effective batch size = 8\n",
    "    save_steps=5000,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    eval_strategy=\"no\",  # No evaluation during training\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=1.0,\n",
    "    logging_first_step=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "490beb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metrics configured\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    mask = labels != -100\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    preds = preds[mask]\n",
    "    labels = labels[mask]\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "print(\"✅ Metrics configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54c1ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleared GPU cache\n",
      "✅ Trainer created successfully (no evaluation during training)\n",
      "\n",
      "Testing trainer dataloader...\n",
      "✓ Dataloader test passed\n",
      "  Batch input_ids shape: torch.Size([1, 256])\n",
      "  GPU memory: 12.30 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1767/2299144124.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "\n",
    "# Clear any stale accelerator state\n",
    "try:\n",
    "    from accelerate.state import AcceleratorState\n",
    "    if hasattr(AcceleratorState, '_shared_state') and AcceleratorState._shared_state:\n",
    "        AcceleratorState._reset_state()\n",
    "        print(\"✓ Cleared accelerator state\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not clear accelerator state: {e}\")\n",
    "\n",
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✓ Cleared GPU cache\")\n",
    "\n",
    "# Create trainer WITHOUT eval_dataset to avoid evaluation during training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset removed - we'll evaluate manually later\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Disable autocast\n",
    "trainer.autocast_smart_context_manager = nullcontext\n",
    "\n",
    "print(\"✅ Trainer created successfully (no evaluation during training)\")\n",
    "\n",
    "# Quick test\n",
    "print(\"\\nTesting trainer dataloader...\")\n",
    "try:\n",
    "    train_dataloader = trainer.get_train_dataloader()\n",
    "    test_batch = next(iter(train_dataloader))\n",
    "    print(f\"✓ Dataloader test passed\")\n",
    "    print(f\"  Batch input_ids shape: {test_batch['input_ids'].shape}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Dataloader test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18b5f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45457e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n",
      "Memory optimization settings:\n",
      "  - Batch size: 2\n",
      "  - Gradient accumulation: 4 steps\n",
      "  - Max sequence length: 256\n",
      "  - Gradient checkpointing: Enabled\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19980' max='19980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19980/19980 36:02, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>17.720500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>18.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>18.230200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>18.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>18.266700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>18.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>18.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>17.784100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>18.084500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>18.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>17.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>17.836500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>17.630500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>17.749500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>17.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>17.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>17.530600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>17.467500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>17.596100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>17.401900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>17.593800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>17.340200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>17.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>17.426100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>17.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>17.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>17.266100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>17.278500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>17.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>16.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>16.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>16.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>17.167800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>17.261500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>16.759800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>16.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>16.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>17.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>17.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>16.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>16.570100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>16.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>16.904300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>16.886300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>16.828200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>16.736200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>16.886600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>16.545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>16.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>16.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>16.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>16.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>16.522900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>16.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>16.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>16.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>16.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>16.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>16.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>16.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>16.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>16.163600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>16.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>16.206900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>16.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>16.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>16.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>16.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>16.246300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>16.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>16.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>16.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>16.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>16.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>15.961300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>15.947400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>16.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>16.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>15.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>15.993400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>15.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>15.906400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>16.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>15.730500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>15.582500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>15.863200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>15.994500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>16.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>15.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>15.854300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>15.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>15.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>15.762600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>15.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>15.701500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>15.905100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>15.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>15.483500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>15.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>15.775400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>15.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>15.229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>15.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>15.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>15.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>15.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>15.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>15.679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>15.116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>15.269800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>15.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>15.082900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>15.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>15.480200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>15.570900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>15.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>15.456200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>15.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>15.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>15.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>15.358700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>15.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>15.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>15.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>15.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>15.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>15.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>15.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>15.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>15.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>14.831200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>14.821300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>15.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>14.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>15.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>15.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>15.279100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>14.934900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>14.915200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>14.937200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>14.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>14.899900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>15.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>15.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>14.715600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>14.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>15.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>15.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>15.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>14.779700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>14.771300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>14.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>15.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>14.930700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>14.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>14.833900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>14.839100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>14.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>14.907900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>14.861600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>14.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>14.946400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>14.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>15.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>14.445300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>15.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>14.599600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>14.843200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>14.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>15.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>14.638600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>14.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>14.555400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>14.725800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>14.725700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>14.714400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>14.574600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>14.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>14.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>14.675300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>14.620200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>14.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>14.215900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>14.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>14.631700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>14.712500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>14.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>14.679100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>14.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>14.875800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>14.517500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>14.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>14.555500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>14.348600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>14.404900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>14.783600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>14.627400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>14.816700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ Training completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "print(\"Memory optimization settings:\")\n",
    "print(f\"  - Batch size: 2\")\n",
    "print(f\"  - Gradient accumulation: 4 steps\")\n",
    "print(f\"  - Max sequence length: 256\")\n",
    "print(f\"  - Gradient checkpointing: Enabled\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"=\"*60)\n",
    "    print(\"✅ Training completed!\")\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e):\n",
    "        print(\"\\n❌ CUDA Out of Memory Error!\")\n",
    "        print(\"Try further reducing:\")\n",
    "        print(\"  1. per_device_train_batch_size to 1\")\n",
    "        print(\"  2. max_length to 128\")\n",
    "        print(\"  3. Or use a smaller model\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2dffaa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING MODEL (BEFORE EVALUATION)\n",
      "============================================================\n",
      "\n",
      "Saving model to ./KinaseFNet_10k...\n",
      "✓ Model saved\n",
      "\n",
      "Saving tokenizer to ./KinaseFNet_10k...\n",
      "✓ Tokenizer saved\n",
      "\n",
      "Saving training arguments...\n",
      "✓ Training args saved\n",
      "\n",
      "============================================================\n",
      "✅ MODEL SAFELY SAVED TO: ./KinaseFNet_10k\n",
      "============================================================\n",
      "\n",
      "Saved files (7):\n",
      "  - checkpoint-19980\n",
      "  - config.json\n",
      "  - model.safetensors\n",
      "  - special_tokens_map.json\n",
      "  - tokenizer.json\n",
      "  - tokenizer_config.json\n",
      "  - training_args.bin\n",
      "\n",
      "✅ Your model is now safely saved!\n",
      "   You can load it later with:\n",
      "   model = AutoModelForMaskedLM.from_pretrained(\"./KinaseFNet_10k\")\n",
      "   tokenizer = AutoTokenizer.from_pretrained(\"./KinaseFNet_10k\")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODEL (BEFORE EVALUATION)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save the trained model immediately\n",
    "output_dir = \"./KinaseFNet_10k\"\n",
    "\n",
    "print(f\"\\nSaving model to {output_dir}...\")\n",
    "trainer.save_model(output_dir)\n",
    "print(\"✓ Model saved\")\n",
    "\n",
    "print(f\"\\nSaving tokenizer to {output_dir}...\")\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"✓ Tokenizer saved\")\n",
    "\n",
    "print(f\"\\nSaving training arguments...\")\n",
    "torch.save(training_args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "print(\"✓ Training args saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"✅ MODEL SAFELY SAVED TO: {output_dir}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify files were saved\n",
    "import os\n",
    "saved_files = os.listdir(output_dir)\n",
    "print(f\"\\nSaved files ({len(saved_files)}):\")\n",
    "for f in sorted(saved_files)[:10]:  # Show first 10 files\n",
    "    print(f\"  - {f}\")\n",
    "if len(saved_files) > 10:\n",
    "    print(f\"  ... and {len(saved_files) - 10} more files\")\n",
    "\n",
    "print(\"\\n✅ Your model is now safely saved!\")\n",
    "print(\"   You can load it later with:\")\n",
    "print(f'   model = AutoModelForMaskedLM.from_pretrained(\"{output_dir}\")')\n",
    "print(f'   tokenizer = AutoTokenizer.from_pretrained(\"{output_dir}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aff53254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from ./KinaseFNet_10k...\n",
      "✓ Model and tokenizer loaded successfully\n",
      "✓ Model forward patched to ignore unsupported arguments\n",
      "\n",
      "============================================================\n",
      "EVALUATING MASKED LANGUAGE MODEL (with dynamic masking)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 253/253 [00:02<00:00, 105.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation Complete\n",
      "------------------------------------------------------------\n",
      "Average Test Loss : 2.2571\n",
      "Perplexity         : 9.56\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import inspect\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MODEL AND TOKENIZER\n",
    "# ============================================================\n",
    "output_dir = \"./KinaseFNet_10k\"\n",
    "\n",
    "print(f\"Loading trained model from {output_dir}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForMaskedLM.from_pretrained(output_dir)\n",
    "print(\"✓ Model and tokenizer loaded successfully\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# ============================================================\n",
    "# PATCH MODEL FOR SAFETY\n",
    "# ============================================================\n",
    "def safe_forward(original_forward):\n",
    "    sig = inspect.signature(original_forward)\n",
    "    valid_keys = set(sig.parameters.keys())\n",
    "    def wrapped_forward(*args, **kwargs):\n",
    "        filtered = {k: v for k, v in kwargs.items() if k in valid_keys}\n",
    "        return original_forward(*args, **filtered)\n",
    "    return wrapped_forward\n",
    "\n",
    "model.forward = safe_forward(model.forward)\n",
    "print(\"✓ Model forward patched to ignore unsupported arguments\")\n",
    "\n",
    "# ============================================================\n",
    "# DATA COLLATOR (for masking during evaluation)\n",
    "# ============================================================\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, collate_fn=data_collator)\n",
    "\n",
    "print(\"\\n============================================================\")\n",
    "print(\"EVALUATING MASKED LANGUAGE MODEL (with dynamic masking)\")\n",
    "print(\"============================================================\")\n",
    "\n",
    "total_loss = 0.0\n",
    "total_count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        if loss is not None:\n",
    "            total_loss += loss.item() * batch[\"input_ids\"].size(0)\n",
    "            total_count += batch[\"input_ids\"].size(0)\n",
    "\n",
    "# ============================================================\n",
    "# RESULTS\n",
    "# ============================================================\n",
    "if total_count > 0:\n",
    "    avg_loss = total_loss / total_count\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    print(\"\\n✅ Evaluation Complete\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(f\"Average Test Loss : {avg_loss:.4f}\")\n",
    "    print(f\"Perplexity         : {perplexity:.2f}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "else:\n",
    "    print(\"⚠️ No valid batches returned loss — check masking or dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b94f37ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from ./KinaseFNet_10k...\n",
      "✓ Model and tokenizer loaded successfully\n",
      "✓ Model forward patched to ignore unsupported arguments\n",
      "\n",
      "Total samples in full dataset: 10,000\n",
      "\n",
      "============================================================\n",
      "EVALUATING MASKED LANGUAGE MODEL ON FULL DATASET\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1250/1250 [00:22<00:00, 56.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation Complete\n",
      "------------------------------------------------------------\n",
      "Average Full Dataset Loss : 1.8505\n",
      "Perplexity                : 6.36\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import inspect\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MODEL AND TOKENIZER\n",
    "# ============================================================\n",
    "output_dir = \"./KinaseFNet_10k\"\n",
    "\n",
    "print(f\"Loading trained model from {output_dir}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForMaskedLM.from_pretrained(output_dir)\n",
    "print(\"✓ Model and tokenizer loaded successfully\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# ============================================================\n",
    "# PATCH MODEL FOR SAFETY (ignore unsupported kwargs)\n",
    "# ============================================================\n",
    "def safe_forward(original_forward):\n",
    "    sig = inspect.signature(original_forward)\n",
    "    valid_keys = set(sig.parameters.keys())\n",
    "    def wrapped_forward(*args, **kwargs):\n",
    "        filtered = {k: v for k, v in kwargs.items() if k in valid_keys}\n",
    "        return original_forward(*args, **filtered)\n",
    "    return wrapped_forward\n",
    "\n",
    "model.forward = safe_forward(model.forward)\n",
    "print(\"✓ Model forward patched to ignore unsupported arguments\")\n",
    "\n",
    "# ============================================================\n",
    "# MERGE DATASETS (train + validation + test)\n",
    "# ============================================================\n",
    "full_dataset = ConcatDataset([train_dataset, val_dataset, test_dataset])\n",
    "print(f\"\\nTotal samples in full dataset: {len(full_dataset):,}\")\n",
    "\n",
    "# ============================================================\n",
    "# DATA COLLATOR (dynamic masking)\n",
    "# ============================================================\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Slightly larger batch for faster evaluation if GPU allows\n",
    "test_loader = DataLoader(full_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "print(\"\\n============================================================\")\n",
    "print(\"EVALUATING MASKED LANGUAGE MODEL ON FULL DATASET\")\n",
    "print(\"============================================================\")\n",
    "\n",
    "total_loss = 0.0\n",
    "total_count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        if loss is not None:\n",
    "            total_loss += loss.item() * batch[\"input_ids\"].size(0)\n",
    "            total_count += batch[\"input_ids\"].size(0)\n",
    "\n",
    "# ============================================================\n",
    "# RESULTS\n",
    "# ============================================================\n",
    "if total_count > 0:\n",
    "    avg_loss = total_loss / total_count\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    print(\"\\n✅ Evaluation Complete\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(f\"Average Full Dataset Loss : {avg_loss:.4f}\")\n",
    "    print(f\"Perplexity                : {perplexity:.2f}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "else:\n",
    "    print(\"⚠️ No valid batches returned loss — check dataset or collator.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafc544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
