{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a335e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in /home/mluser/.venv/lib/python3.12/site-packages (1.85)\n",
      "Requirement already satisfied: transformers in /home/mluser/.venv/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /home/mluser/.venv/lib/python3.12/site-packages (2.9.0)\n",
      "Requirement already satisfied: datasets in /home/mluser/.venv/lib/python3.12/site-packages (4.2.0)\n",
      "Requirement already satisfied: numpy in /home/mluser/.venv/lib/python3.12/site-packages (2.3.4)\n",
      "Requirement already satisfied: scikit-learn in /home/mluser/.venv/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: evaluate in /home/mluser/.venv/lib/python3.12/site-packages (0.4.6)\n",
      "Requirement already satisfied: filelock in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/mluser/.venv/lib/python3.12/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/mluser/.venv/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/mluser/.venv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/mluser/.venv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/mluser/.venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/mluser/.venv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/mluser/.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/mluser/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: anyio in /home/mluser/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/mluser/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/mluser/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/mluser/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/mluser/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/mluser/.venv/lib/python3.12/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/mluser/.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/mluser/.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/mluser/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/mluser/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/mluser/.venv/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mluser/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mluser/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mluser/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/mluser/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/mluser/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install biopython transformers torch datasets numpy scikit-learn evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6020c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /home/mluser/.venv/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.2 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (2.9.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /home/mluser/.venv/lib/python3.12/site-packages (from transformers[torch]) (1.10.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/mluser/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.1.10)\n",
      "Requirement already satisfied: psutil in /home/mluser/.venv/lib/python3.12/site-packages (from accelerate>=0.26.0->transformers[torch]) (7.1.0)\n",
      "Requirement already satisfied: setuptools in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/mluser/.venv/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/mluser/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mluser/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mluser/.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59246fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mluser/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "import evaluate\n",
    "import inspect\n",
    "\n",
    "print(\"✅ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e895d205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU cache cleared\n",
      "   GPU: NVIDIA GeForce RTX 4090\n",
      "   Total GPU memory: 25.25 GB\n",
      "   Available memory: 25.25 GB\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache before starting\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"✅ GPU cache cleared\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   Available memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca836fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/fnet-base\")\n",
    "print(\"✅ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3750af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FASTADataset class defined\n"
     ]
    }
   ],
   "source": [
    "class FASTADataset(Dataset):\n",
    "    def __init__(self, fasta_file, tokenizer, max_length=512):\n",
    "        print(f\"Loading sequences from {fasta_file}...\")\n",
    "        self.sequences = []\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            seq = str(record.seq)\n",
    "            if len(seq) > 0:\n",
    "                self.sequences.append(seq)\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        print(f\"Loaded {len(self.sequences)} sequences\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        spaced_seq = \" \".join(list(seq))\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            spaced_seq,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        result = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        if 'attention_mask' not in result:\n",
    "            result['attention_mask'] = torch.ones_like(result['input_ids'])\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"✅ FASTADataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2735ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found: /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_train_10k.fasta\n",
      "✓ Found: /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_val_10k.fasta\n",
      "✓ Found: /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_test_10k.fasta\n",
      "Loading sequences from /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_train_10k.fasta...\n",
      "Loaded 7989 sequences\n",
      "Loading sequences from /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_val_10k.fasta...\n",
      "Loaded 1002 sequences\n",
      "Loading sequences from /home/mluser/AFML_RISHABH/Project/10k sequences/kinases_cluster_test_10k.fasta...\n",
      "Loaded 1009 sequences\n",
      "\n",
      "==================================================\n",
      "Dataset sizes: 7989, 1002, 1009\n",
      "Max sequence length: 512\n",
      "==================================================\n",
      "\n",
      "Testing dataset[0]...\n",
      "✓ Sample retrieved successfully\n",
      "  Keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "  input_ids shape: torch.Size([512])\n",
      "  First 20 tokens: [4, 94, 123, 100, 153, 266, 101, 66, 129, 66, 66, 66, 66, 66, 66, 66, 101, 70, 101, 66]\n"
     ]
    }
   ],
   "source": [
    "data_folder = '/home/mluser/AFML_RISHABH/Project/10k sequences'\n",
    "\n",
    "train_path = os.path.join(data_folder, \"kinases_cluster_train_10k.fasta\")\n",
    "val_path   = os.path.join(data_folder, \"kinases_cluster_val_10k.fasta\")\n",
    "test_path  = os.path.join(data_folder, \"kinases_cluster_test_10k.fasta\")\n",
    "\n",
    "# Verify files exist\n",
    "for path in [train_path, val_path, test_path]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    print(f\"✓ Found: {path}\")\n",
    "\n",
    "# Use reduced max_length to save memory\n",
    "MAX_LENGTH = 512  # Reduced from 512\n",
    "\n",
    "train_dataset = FASTADataset(train_path, tokenizer, max_length=MAX_LENGTH)\n",
    "val_dataset   = FASTADataset(val_path, tokenizer, max_length=MAX_LENGTH)\n",
    "test_dataset  = FASTADataset(test_path, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Dataset sizes: {len(train_dataset)}, {len(val_dataset)}, {len(test_dataset)}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test dataset\n",
    "print(\"\\nTesting dataset[0]...\")\n",
    "sample = train_dataset[0]\n",
    "print(\"✓ Sample retrieved successfully\")\n",
    "print(f\"  Keys: {sample.keys()}\")\n",
    "print(f\"  input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"  First 20 tokens: {sample['input_ids'][:20].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e4fa0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gradient checkpointing enabled\n",
      "✅ Model ready on cuda\n",
      "   GPU memory allocated: 0.33 GB\n",
      "   GPU memory reserved: 0.37 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mluser/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.fnet.modeling_fnet import FNetBasicFourierTransform\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"google/fnet-base\")\n",
    "\n",
    "# Patch FNet Fourier Transform for float32\n",
    "class FNetSafeFourierTransform(FNetBasicFourierTransform):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        outputs = torch.fft.fftn(hidden_states, dim=(-2, -1)).real\n",
    "        return (outputs,)\n",
    "\n",
    "model.fourier_transform = FNetSafeFourierTransform(model.config)\n",
    "\n",
    "# Force FP32 and handle unexpected kwargs\n",
    "def force_fp32_forward(original_forward):\n",
    "    sig = inspect.signature(original_forward)\n",
    "    \n",
    "    def wrapped_forward(*args, **kwargs):\n",
    "        valid_params = set(sig.parameters.keys())\n",
    "        filtered_kwargs = {k: v for k, v in kwargs.items() if k in valid_params}\n",
    "        \n",
    "        with torch.autocast(device_type='cuda', enabled=False):\n",
    "            return original_forward(*args, **filtered_kwargs)\n",
    "    \n",
    "    return wrapped_forward\n",
    "\n",
    "model.forward = force_fp32_forward(model.forward)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).to(torch.float32)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"✓ Gradient checkpointing enabled\")\n",
    "\n",
    "print(f\"✅ Model ready on {device}\")\n",
    "\n",
    "# Check memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"   GPU memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2700607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data collator...\n",
      "✓ Collated batch keys: KeysView({'input_ids': tensor([[  4,  94,   6,  ..., 101, 101,   5],\n",
      "        [  4,  94, 123,  ..., 101, 164,   5]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[-100, -100,  123,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]])})\n",
      "  input_ids shape: torch.Size([2, 512])\n",
      "  labels shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Test the data collator\n",
    "print(\"Testing data collator...\")\n",
    "batch = [train_dataset[i] for i in range(2)]\n",
    "collated = data_collator(batch)\n",
    "print(f\"✓ Collated batch keys: {collated.keys()}\")\n",
    "print(f\"  input_ids shape: {collated['input_ids'].shape}\")\n",
    "print(f\"  labels shape: {collated['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a97ac80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./KinaseFNet_10k_512\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=1,  # Batch size 1\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # Keep effective batch size = 8\n",
    "    save_steps=5000,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    eval_strategy=\"no\",  # No evaluation during training\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=1.0,\n",
    "    logging_first_step=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "490beb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metrics configured\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    mask = labels != -100\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    preds = preds[mask]\n",
    "    labels = labels[mask]\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "print(\"✅ Metrics configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54c1ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleared GPU cache\n",
      "✅ Trainer created successfully (no evaluation during training)\n",
      "\n",
      "Testing trainer dataloader...\n",
      "✓ Dataloader test passed\n",
      "  Batch input_ids shape: torch.Size([1, 512])\n",
      "  GPU memory: 0.33 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3005/2299144124.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "\n",
    "# Clear any stale accelerator state\n",
    "try:\n",
    "    from accelerate.state import AcceleratorState\n",
    "    if hasattr(AcceleratorState, '_shared_state') and AcceleratorState._shared_state:\n",
    "        AcceleratorState._reset_state()\n",
    "        print(\"✓ Cleared accelerator state\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not clear accelerator state: {e}\")\n",
    "\n",
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✓ Cleared GPU cache\")\n",
    "\n",
    "# Create trainer WITHOUT eval_dataset to avoid evaluation during training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset removed - we'll evaluate manually later\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Disable autocast\n",
    "trainer.autocast_smart_context_manager = nullcontext\n",
    "\n",
    "print(\"✅ Trainer created successfully (no evaluation during training)\")\n",
    "\n",
    "# Quick test\n",
    "print(\"\\nTesting trainer dataloader...\")\n",
    "try:\n",
    "    train_dataloader = trainer.get_train_dataloader()\n",
    "    test_batch = next(iter(train_dataloader))\n",
    "    print(f\"✓ Dataloader test passed\")\n",
    "    print(f\"  Batch input_ids shape: {test_batch['input_ids'].shape}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Dataloader test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18b5f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45457e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n",
      "Memory optimization settings:\n",
      "  - Batch size: 2\n",
      "  - Gradient accumulation: 4 steps\n",
      "  - Max sequence length: 256\n",
      "  - Gradient checkpointing: Enabled\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19980' max='19980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19980/19980 41:00, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>31.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>21.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>20.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>20.557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>20.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>20.186200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>19.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>19.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>19.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>19.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>19.283200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>19.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>19.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>19.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>18.959400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>18.896200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>18.855200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>18.724400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>18.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>18.634300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>18.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>18.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>18.392500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>18.344300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>18.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>18.330400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>18.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>18.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>18.268300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>18.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>17.916700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>17.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>17.855600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>17.962600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>17.924900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>17.747700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>17.690600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>17.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>17.830600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>17.770400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>17.803700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>17.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>17.507200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>17.621800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>17.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>17.551700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>17.426500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>17.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>17.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>17.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>17.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>17.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>17.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>17.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>17.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>17.264200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>17.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>17.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>17.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>17.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>17.155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>17.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>17.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>16.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>16.930800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>16.857200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>16.733100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>17.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>16.925300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>16.896300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>16.826700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>16.989300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>16.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>16.869700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>16.742800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>16.605500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>16.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>16.784900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>16.526100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>16.587500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>16.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>16.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>16.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>16.739100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>16.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>16.330900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>16.503700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>16.616500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>16.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>16.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>16.457500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>16.350800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>16.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>16.508700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>16.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>16.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>16.579000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>16.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>16.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>16.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>16.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>16.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>15.970900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>16.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>16.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>16.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>16.330300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>16.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>16.187400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>15.847900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>16.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>15.998800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>15.932300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>16.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>16.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>16.176800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>16.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>15.972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>16.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>16.102700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>15.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>16.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>15.811800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>15.885600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>15.845300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>15.962400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>15.854300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>16.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>15.936100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>15.866600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>15.813400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>15.925700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>15.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>15.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>15.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>15.719700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>15.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>15.820900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>15.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>15.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>15.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>15.627400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>15.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>15.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>15.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>15.770900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>15.579300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>15.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>15.792100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>15.916500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>15.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>15.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>15.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>15.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>15.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>15.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>15.477800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>15.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>15.710300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>15.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>15.613200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>15.576400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>15.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>15.645300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>15.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>15.796100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>15.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>15.665400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>15.394100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>15.707700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>15.411600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>15.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>15.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>15.543700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>15.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>15.464500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>15.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>15.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>15.357300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>15.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>15.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>15.403000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>15.354600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>15.489100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>15.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>15.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>15.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>15.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>15.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>15.423900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>15.505200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>15.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>15.332300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>15.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>15.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>15.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>15.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>15.464800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>15.395100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>15.562900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ Training completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "print(\"Memory optimization settings:\")\n",
    "print(f\"  - Batch size: 2\")\n",
    "print(f\"  - Gradient accumulation: 4 steps\")\n",
    "print(f\"  - Max sequence length: 256\")\n",
    "print(f\"  - Gradient checkpointing: Enabled\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"=\"*60)\n",
    "    print(\"✅ Training completed!\")\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e):\n",
    "        print(\"\\n❌ CUDA Out of Memory Error!\")\n",
    "        print(\"Try further reducing:\")\n",
    "        print(\"  1. per_device_train_batch_size to 1\")\n",
    "        print(\"  2. max_length to 128\")\n",
    "        print(\"  3. Or use a smaller model\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dffaa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING MODEL (BEFORE EVALUATION)\n",
      "============================================================\n",
      "\n",
      "Saving model to ./KinaseFNet_10k_512...\n",
      "✓ Model saved\n",
      "\n",
      "Saving tokenizer to ./KinaseFNet_10k_512...\n",
      "✓ Tokenizer saved\n",
      "\n",
      "Saving training arguments...\n",
      "✓ Training args saved\n",
      "\n",
      "============================================================\n",
      "✅ MODEL SAFELY SAVED TO: ./KinaseFNet_10k_512\n",
      "============================================================\n",
      "\n",
      "Saved files (7):\n",
      "  - checkpoint-19980\n",
      "  - config.json\n",
      "  - model.safetensors\n",
      "  - special_tokens_map.json\n",
      "  - tokenizer.json\n",
      "  - tokenizer_config.json\n",
      "  - training_args.bin\n",
      "\n",
      "✅ Your model is now safely saved!\n",
      "   You can load it later with:\n",
      "   model = AutoModelForMaskedLM.from_pretrained(\"./KinaseFNet_10k_512\")\n",
      "   tokenizer = AutoTokenizer.from_pretrained(\"./KinaseFNet_10k_512\")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODEL (BEFORE EVALUATION)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save the trained model immediately\n",
    "output_dir = \"./KinaseFNet_10k_512\"\n",
    "\n",
    "print(f\"\\nSaving model to {output_dir}...\")\n",
    "trainer.save_model(output_dir)\n",
    "print(\"✓ Model saved\")\n",
    "\n",
    "print(f\"\\nSaving tokenizer to {output_dir}...\")\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"✓ Tokenizer saved\")\n",
    "\n",
    "print(f\"\\nSaving training arguments...\")\n",
    "torch.save(training_args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "print(\"✓ Training args saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"✅ MODEL SAFELY SAVED TO: {output_dir}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify files were saved\n",
    "import os\n",
    "saved_files = os.listdir(output_dir)\n",
    "print(f\"\\nSaved files ({len(saved_files)}):\")\n",
    "for f in sorted(saved_files)[:10]:  # Show first 10 files\n",
    "    print(f\"  - {f}\")\n",
    "if len(saved_files) > 10:\n",
    "    print(f\"  ... and {len(saved_files) - 10} more files\")\n",
    "\n",
    "print(\"\\n✅ Your model is now safely saved!\")\n",
    "print(\"   You can load it later with:\")\n",
    "print(f'   model = AutoModelForMaskedLM.from_pretrained(\"{output_dir}\")')\n",
    "print(f'   tokenizer = AutoTokenizer.from_pretrained(\"{output_dir}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aff53254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from ./KinaseFNet_10k_512...\n",
      "✓ Model and tokenizer loaded successfully\n",
      "✓ Model forward patched to ignore unsupported arguments\n",
      "\n",
      "============================================================\n",
      "EVALUATING MASKED LANGUAGE MODEL (with dynamic masking)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 253/253 [00:03<00:00, 71.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation Complete\n",
      "------------------------------------------------------------\n",
      "Average Test Loss : 2.2356\n",
      "Perplexity         : 9.35\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import inspect\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MODEL AND TOKENIZER\n",
    "# ============================================================\n",
    "output_dir = \"./KinaseFNet_10k_512\"\n",
    "\n",
    "print(f\"Loading trained model from {output_dir}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForMaskedLM.from_pretrained(output_dir)\n",
    "print(\"✓ Model and tokenizer loaded successfully\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# ============================================================\n",
    "# PATCH MODEL FOR SAFETY\n",
    "# ============================================================\n",
    "def safe_forward(original_forward):\n",
    "    sig = inspect.signature(original_forward)\n",
    "    valid_keys = set(sig.parameters.keys())\n",
    "    def wrapped_forward(*args, **kwargs):\n",
    "        filtered = {k: v for k, v in kwargs.items() if k in valid_keys}\n",
    "        return original_forward(*args, **filtered)\n",
    "    return wrapped_forward\n",
    "\n",
    "model.forward = safe_forward(model.forward)\n",
    "print(\"✓ Model forward patched to ignore unsupported arguments\")\n",
    "\n",
    "# ============================================================\n",
    "# DATA COLLATOR (for masking during evaluation)\n",
    "# ============================================================\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, collate_fn=data_collator)\n",
    "\n",
    "print(\"\\n============================================================\")\n",
    "print(\"EVALUATING MASKED LANGUAGE MODEL (with dynamic masking)\")\n",
    "print(\"============================================================\")\n",
    "\n",
    "total_loss = 0.0\n",
    "total_count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        if loss is not None:\n",
    "            total_loss += loss.item() * batch[\"input_ids\"].size(0)\n",
    "            total_count += batch[\"input_ids\"].size(0)\n",
    "\n",
    "# ============================================================\n",
    "# RESULTS\n",
    "# ============================================================\n",
    "if total_count > 0:\n",
    "    avg_loss = total_loss / total_count\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    print(\"\\n✅ Evaluation Complete\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(f\"Average Test Loss : {avg_loss:.4f}\")\n",
    "    print(f\"Perplexity         : {perplexity:.2f}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "else:\n",
    "    print(\"⚠️ No valid batches returned loss — check masking or dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b94f37ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from ./KinaseFNet_10k_512...\n",
      "✓ Model and tokenizer loaded successfully\n",
      "✓ Model forward patched to ignore unsupported arguments\n",
      "\n",
      "Total samples in full dataset: 10,000\n",
      "\n",
      "============================================================\n",
      "EVALUATING MASKED LANGUAGE MODEL ON FULL DATASET\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1250/1250 [00:36<00:00, 33.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation Complete\n",
      "------------------------------------------------------------\n",
      "Average Full Dataset Loss : 1.9496\n",
      "Perplexity                : 7.03\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import inspect\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MODEL AND TOKENIZER\n",
    "# ============================================================\n",
    "output_dir = \"./KinaseFNet_10k_512\"\n",
    "\n",
    "print(f\"Loading trained model from {output_dir}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForMaskedLM.from_pretrained(output_dir)\n",
    "print(\"✓ Model and tokenizer loaded successfully\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# ============================================================\n",
    "# PATCH MODEL FOR SAFETY (ignore unsupported kwargs)\n",
    "# ============================================================\n",
    "def safe_forward(original_forward):\n",
    "    sig = inspect.signature(original_forward)\n",
    "    valid_keys = set(sig.parameters.keys())\n",
    "    def wrapped_forward(*args, **kwargs):\n",
    "        filtered = {k: v for k, v in kwargs.items() if k in valid_keys}\n",
    "        return original_forward(*args, **filtered)\n",
    "    return wrapped_forward\n",
    "\n",
    "model.forward = safe_forward(model.forward)\n",
    "print(\"✓ Model forward patched to ignore unsupported arguments\")\n",
    "\n",
    "# ============================================================\n",
    "# MERGE DATASETS (train + validation + test)\n",
    "# ============================================================\n",
    "full_dataset = ConcatDataset([train_dataset, val_dataset, test_dataset])\n",
    "print(f\"\\nTotal samples in full dataset: {len(full_dataset):,}\")\n",
    "\n",
    "# ============================================================\n",
    "# DATA COLLATOR (dynamic masking)\n",
    "# ============================================================\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Slightly larger batch for faster evaluation if GPU allows\n",
    "test_loader = DataLoader(full_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "print(\"\\n============================================================\")\n",
    "print(\"EVALUATING MASKED LANGUAGE MODEL ON FULL DATASET\")\n",
    "print(\"============================================================\")\n",
    "\n",
    "total_loss = 0.0\n",
    "total_count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        if loss is not None:\n",
    "            total_loss += loss.item() * batch[\"input_ids\"].size(0)\n",
    "            total_count += batch[\"input_ids\"].size(0)\n",
    "\n",
    "# ============================================================\n",
    "# RESULTS\n",
    "# ============================================================\n",
    "if total_count > 0:\n",
    "    avg_loss = total_loss / total_count\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    print(\"\\n✅ Evaluation Complete\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(f\"Average Full Dataset Loss : {avg_loss:.4f}\")\n",
    "    print(f\"Perplexity                : {perplexity:.2f}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "else:\n",
    "    print(\"⚠️ No valid batches returned loss — check dataset or collator.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafc544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
